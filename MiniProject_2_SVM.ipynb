{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "39d1505b",
   "metadata": {},
   "source": [
    "#### Applied Machine Learning- Mini Project 2 (Tasnim Ahmed - ta1743)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a63b60",
   "metadata": {},
   "source": [
    "## Support Vector Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9adb5a4",
   "metadata": {},
   "source": [
    "### Exploring the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "79244f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "79aee326",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer  # we will be working with breast cancer dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "21dcf3ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cancer = load_breast_cancer()          #load the data \n",
    "df_cancer = pd.DataFrame(data_cancer.data, columns=data_cancer.feature_names) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "583ad29c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean radius</th>\n",
       "      <th>mean texture</th>\n",
       "      <th>mean perimeter</th>\n",
       "      <th>mean area</th>\n",
       "      <th>mean smoothness</th>\n",
       "      <th>mean compactness</th>\n",
       "      <th>mean concavity</th>\n",
       "      <th>mean concave points</th>\n",
       "      <th>mean symmetry</th>\n",
       "      <th>mean fractal dimension</th>\n",
       "      <th>...</th>\n",
       "      <th>worst texture</th>\n",
       "      <th>worst perimeter</th>\n",
       "      <th>worst area</th>\n",
       "      <th>worst smoothness</th>\n",
       "      <th>worst compactness</th>\n",
       "      <th>worst concavity</th>\n",
       "      <th>worst concave points</th>\n",
       "      <th>worst symmetry</th>\n",
       "      <th>worst fractal dimension</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>0.07871</td>\n",
       "      <td>...</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>0.05667</td>\n",
       "      <td>...</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>0.05999</td>\n",
       "      <td>...</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.2414</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>0.09744</td>\n",
       "      <td>...</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.1980</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>0.05883</td>\n",
       "      <td>...</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n",
       "0        17.99         10.38          122.80     1001.0          0.11840   \n",
       "1        20.57         17.77          132.90     1326.0          0.08474   \n",
       "2        19.69         21.25          130.00     1203.0          0.10960   \n",
       "3        11.42         20.38           77.58      386.1          0.14250   \n",
       "4        20.29         14.34          135.10     1297.0          0.10030   \n",
       "\n",
       "   mean compactness  mean concavity  mean concave points  mean symmetry  \\\n",
       "0           0.27760          0.3001              0.14710         0.2419   \n",
       "1           0.07864          0.0869              0.07017         0.1812   \n",
       "2           0.15990          0.1974              0.12790         0.2069   \n",
       "3           0.28390          0.2414              0.10520         0.2597   \n",
       "4           0.13280          0.1980              0.10430         0.1809   \n",
       "\n",
       "   mean fractal dimension  ...  worst texture  worst perimeter  worst area  \\\n",
       "0                 0.07871  ...          17.33           184.60      2019.0   \n",
       "1                 0.05667  ...          23.41           158.80      1956.0   \n",
       "2                 0.05999  ...          25.53           152.50      1709.0   \n",
       "3                 0.09744  ...          26.50            98.87       567.7   \n",
       "4                 0.05883  ...          16.67           152.20      1575.0   \n",
       "\n",
       "   worst smoothness  worst compactness  worst concavity  worst concave points  \\\n",
       "0            0.1622             0.6656           0.7119                0.2654   \n",
       "1            0.1238             0.1866           0.2416                0.1860   \n",
       "2            0.1444             0.4245           0.4504                0.2430   \n",
       "3            0.2098             0.8663           0.6869                0.2575   \n",
       "4            0.1374             0.2050           0.4000                0.1625   \n",
       "\n",
       "   worst symmetry  worst fractal dimension  Target  \n",
       "0          0.4601                  0.11890       0  \n",
       "1          0.2750                  0.08902       0  \n",
       "2          0.3613                  0.08758       0  \n",
       "3          0.6638                  0.17300       0  \n",
       "4          0.2364                  0.07678       0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cancer['Target'] = data_cancer.target    #loading the target of the dataset\n",
    "df_cancer.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c0e2555e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_cancer.drop('Target', axis = 'columns') #independent variables\n",
    "y = df_cancer['Target']  #dependent variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b1213043",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(569, 30)\n",
      "(569,)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da081d63",
   "metadata": {},
   "source": [
    "### Splitting and Scaling the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e5ea8f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the required modules to split and scale the data\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "a100af1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the data into training (80%) and testing set (20%)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 10, test_size = 0.2)\n",
    "\n",
    "X_train = X_train.to_numpy()\n",
    "X_test = X_test.to_numpy()\n",
    "y_train = y_train.to_numpy()\n",
    "y_test = y_test.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c78719b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling the training data\n",
    "sc = StandardScaler()\n",
    "X_train_tr = sc.fit_transform(X_train)\n",
    "# Adding the bias feature\n",
    "X_train_tr = np.append(arr = np.ones([X_train_tr.shape[0], 1]).astype(int), values = X_train_tr, axis= 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "15462f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling the testing data\n",
    "X_test_tr = sc.transform(X_test)\n",
    "# Adding the bias feature\n",
    "X_test_tr = np.append(arr = np.ones([X_test_tr.shape[0], 1]).astype(int), values = X_test_tr, axis= 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16cf6d58",
   "metadata": {},
   "source": [
    "### Labelling the y Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "44456ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#relabel the y values of the training set\n",
    "y_train_labelled = np.array([(2*y_train[i] - 1) for i in range(len((y_train)))])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0658e15",
   "metadata": {},
   "source": [
    "### Implementing the SVM Primal Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "b37054cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def svm_gradient(X, y, theta, C):\n",
    "    gradient = np.zeros(X.shape[1])\n",
    "    for i in range(len(X)):\n",
    "        if max(0, 1 - y[i]*(np.dot(X[i], theta))) == 0:\n",
    "            gradient += theta\n",
    "        else: \n",
    "            gradient += (theta - (C*y[i]*X[i]))\n",
    "    return (gradient/len(X))\n",
    "\n",
    "def svm_theta(X, y, lr, n_iters, C):\n",
    "    theta = np.random.randn(X.shape[1])    #starting with a random theta\n",
    "    for i in range(n_iters):\n",
    "        sample_indices = np.random.choice(len(X), size = 100, replace = False)\n",
    "        X_sample = X[sample_indices, :]\n",
    "        y_sample = y[sample_indices]\n",
    "        gradient = svm_gradient(X_sample, y_sample, theta, C)\n",
    "        theta = theta - lr*gradient\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "5f353c21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Theta value (w): [ 0.07136031 -0.09446261 -0.11773046 -0.09450721 -0.09417908  0.02987496\n",
      " -0.02706763 -0.06966478 -0.06807671 -0.0524938   0.05919726 -0.06199505\n",
      " -0.00353264 -0.05717399 -0.05895971  0.04315824  0.00451667 -0.00571681\n",
      " -0.02615108  0.00364898  0.02460134 -0.09843924 -0.11419171 -0.09762726\n",
      " -0.09525686  0.00476032 -0.05555936 -0.07630493 -0.08623195 -0.10148702\n",
      " -0.01962416]\n"
     ]
    }
   ],
   "source": [
    "#initial hyperparameters                     \n",
    "lr = 0.5                                       # learning rate\n",
    "n_iters = 1000                                 # number of iterations\n",
    "C = 0.9                                        # lower value of C prevent the model from overfitting\n",
    "#model_theta = svm_gradient(X_train_tr, theta, y_train, C)\n",
    "model_theta = svm_theta(X_train_tr, y_train_labelled, lr, n_iters, C)  \n",
    "print(\"Theta value (w):\", model_theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ffdc88",
   "metadata": {},
   "source": [
    "#### Which training points are closest to the decision boundary?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "190fb138",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The minimum distance from the decision boundary is: 0.017419329951858124\n",
      "Which corresponds to the following observation from X: \n",
      " [1.742e+01 2.556e+01 1.145e+02 9.480e+02 1.006e-01 1.146e-01 1.682e-01\n",
      " 6.597e-02 1.308e-01 5.866e-02 5.296e-01 1.667e+00 3.767e+00 5.853e+01\n",
      " 3.113e-02 8.555e-02 1.438e-01 3.927e-02 2.175e-02 1.256e-02 1.807e+01\n",
      " 2.807e+01 1.204e+02 1.021e+03 1.243e-01 1.793e-01 2.803e-01 1.099e-01\n",
      " 1.603e-01 6.818e-02]\n"
     ]
    }
   ],
   "source": [
    "distance = np.absolute(np.dot(X_train_tr, model_theta))/np.sqrt(np.dot(model_theta, model_theta))\n",
    "min_distance = min(distance)\n",
    "min_index = np.argmin(distance)\n",
    "\n",
    "print(\"The minimum distance from the decision boundary is:\", min_distance)\n",
    "print(\"Which corresponds to the following observation from X: \\n\", X_train[min_index, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ba63f5",
   "metadata": {},
   "source": [
    "#### The decision function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "2fa1a6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_index = np.random.choice(len(X_train_tr), size = 1, replace = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "c67035ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.07136031  0.00906609 -0.00049644  0.01556608  0.016511   -0.03631761\n",
      "   0.02719185  0.05274312  0.05124622  0.04622128 -0.05861315  0.01747146\n",
      "   0.00184575  0.02367727  0.01604921 -0.04719766 -0.00384646  0.00382324\n",
      "   0.02443464 -0.00466171 -0.01554512  0.00029911 -0.01593439  0.01157965\n",
      "   0.01087957 -0.00572885  0.04147204  0.03992407  0.04642337  0.095058\n",
      "   0.00895083]]\n"
     ]
    }
   ],
   "source": [
    "mult = X_train_tr[random_index]*model_theta\n",
    "print(mult)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "815f57a8",
   "metadata": {},
   "source": [
    "#### Predicting the y-values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "017a9408",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.dot(X_test_tr, model_theta)\n",
    "y_predicted = [1 if i >= 0 else 0 for i in y_pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "86c152c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "c08f0313",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 1.0\n",
      "Recall: 0.9733333333333334\n",
      "F1-Score 0.9864864864864865\n",
      "Confusion Matrix: \n",
      " [[39  0]\n",
      " [ 2 73]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Precision:\", precision_score(y_test, y_predicted))\n",
    "print(\"Recall:\", recall_score(y_test, y_predicted))\n",
    "print(\"F1-Score\", f1_score(y_test, y_predicted))\n",
    "print(\"Confusion Matrix: \\n\", confusion_matrix(y_test, y_predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ced0e7",
   "metadata": {},
   "source": [
    "### Testing with blob generated data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f2fcbfd",
   "metadata": {},
   "source": [
    "#### Generating the blob data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "dd94fda8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "X_blob, y_blob = make_blobs(n_samples=1000, centers=2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "8d20d734",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the blob data into training (80%) and testing set (20%)\n",
    "Xb_train, Xb_test, yb_train, yb_test = train_test_split(X_blob, y_blob, random_state = 10, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "38ae2c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling the blob training data\n",
    "scb = StandardScaler()\n",
    "Xb_train_tr = scb.fit_transform(Xb_train)\n",
    "# Adding the bias feature\n",
    "Xb_train_tr = np.append(arr = np.ones([Xb_train_tr.shape[0], 1]).astype(int), values = Xb_train_tr, axis= 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "ece7197e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling the blob testing data\n",
    "Xb_test_tr = scb.transform(Xb_test)\n",
    "# Adding the bias feature\n",
    "Xb_test_tr = np.append(arr = np.ones([Xb_test_tr.shape[0], 1]).astype(int), values = Xb_test_tr, axis= 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "751e4a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "yb_train_labelled = np.array([(2*yb_train[i] - 1) for i in range(len((yb_train)))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "9f2da661",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Theta value (w): [ 0.08494489  0.23154898 -0.63483156]\n"
     ]
    }
   ],
   "source": [
    "#initial hyperparameters                     \n",
    "lr = 0.5                                       # learning rate\n",
    "n_iters = 1000                                 # number of iterations\n",
    "C = 0.9                                        # lower value of C prevent the model from overfitting\n",
    "theta = 1\n",
    "model_theta_blob = svm_theta(Xb_train_tr, yb_train_labelled, lr, n_iters, C)  \n",
    "print(\"Theta value (w):\", model_theta_blob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "23448d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "yb_pred = np.dot(Xb_test_tr, model_theta_blob)\n",
    "yb_predicted = [1 if i >= 0 else 0 for i in yb_pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "c28881ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.9433962264150944\n",
      "Recall: 0.9523809523809523\n",
      "F1-Score 0.9478672985781991\n",
      "Confusion Matrix: \n",
      " [[ 89   6]\n",
      " [  5 100]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Precision:\", precision_score(yb_test, yb_predicted))\n",
    "print(\"Recall:\", recall_score(yb_test, yb_predicted))\n",
    "print(\"F1-Score\", f1_score(yb_test, yb_predicted))\n",
    "print(\"Confusion Matrix: \\n\", confusion_matrix(yb_test, yb_predicted))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
