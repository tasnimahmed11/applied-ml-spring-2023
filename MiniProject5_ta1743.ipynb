{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df2dd00d",
   "metadata": {
    "id": "df2dd00d"
   },
   "source": [
    "#### Applied Machine Learning - Mini Project 5 (Tasnim Ahmed, ta1743)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b2b00a47",
   "metadata": {
    "id": "b2b00a47"
   },
   "outputs": [],
   "source": [
    "import sklearn \n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c819be92",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Avoid printing out warnings\n",
    "with warnings.catch_warnings():\n",
    "     warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe041098",
   "metadata": {
    "id": "fe041098"
   },
   "source": [
    "#### Loading the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c9e0db3",
   "metadata": {
    "id": "0c9e0db3"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits   \n",
    "digits = load_digits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "67ad5e3e",
   "metadata": {
    "id": "67ad5e3e"
   },
   "outputs": [],
   "source": [
    "X = digits.data   # shape = (1797, 64)\n",
    "y = digits.target # shape = (1797,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f832694a",
   "metadata": {
    "id": "f832694a"
   },
   "source": [
    "#### Visualizing the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ad2b458",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 473
    },
    "id": "6ad2b458",
    "outputId": "b58751be-722b-4f25-811a-efe5c758d228"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f9a9b7ca700>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPoAAAECCAYAAADXWsr9AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAL40lEQVR4nO3dW4hd9RXH8d+vY7xGSaxWJBHtSAmIUHNBKgFpNYpWsS81RFCotCQPrRha0NiX4ptPYh+KELxU8IajBoq01gQVEVrtTIz1MrFoiJhEHSWRGAsR4+rD2SkxnTp7xv3/z5mzvh845MzMmb3WzOR39t7n7L2XI0IABtu3ZrsBAOURdCABgg4kQNCBBAg6kABBBxLoi6DbvsL2W7bftr2hcK37bE/Yfr1knSPqnWX7Odvjtt+wfXPhesfbftn2q02920vWa2oO2X7F9lOlazX1dtp+zfY226OFay2w/bjt7c3f8KKCtZY0P9Ph237b6ztZeETM6k3SkKR3JA1LOlbSq5LOK1jvYknLJL1e6ec7U9Ky5v7Jkv5V+OezpPnN/XmSXpL0g8I/468lPSzpqUq/052STqtU6wFJv2juHytpQaW6Q5I+kHR2F8vrhzX6hZLejogdEfG5pEcl/aRUsYh4QdLeUsufpN77EbG1uf+ppHFJiwrWi4g40Hw4r7kVOyrK9mJJV0m6p1SN2WL7FPVWDPdKUkR8HhGfVCp/qaR3IuLdLhbWD0FfJOm9Iz7epYJBmE22z5G0VL21bMk6Q7a3SZqQtDkiSta7S9Itkr4sWONoIekZ22O21xasMyzpI0n3N7sm99g+qWC9I62R9EhXC+uHoHuSzw3ccbm250t6QtL6iNhfslZEHIqICyQtlnSh7fNL1LF9taSJiBgrsfyvsTIilkm6UtIvbV9cqM4x6u3m3R0RSyV9Jqnoa0iSZPtYSddIGulqmf0Q9F2Szjri48WS9sxSL0XYnqdeyB+KiCdr1W02M5+XdEWhEislXWN7p3q7XJfYfrBQrf+KiD3NvxOSNqm3+1fCLkm7jtgiely94Jd2paStEfFhVwvsh6D/Q9L3bH+3eSZbI+lPs9xTZ2xbvX288Yi4s0K9020vaO6fIGmVpO0lakXEbRGxOCLOUe/v9mxEXF+i1mG2T7J98uH7ki6XVOQdlIj4QNJ7tpc0n7pU0pslah3lOnW42S71Nk1mVUR8YftXkv6q3iuN90XEG6Xq2X5E0g8lnWZ7l6TfRcS9peqpt9a7QdJrzX6zJP02Iv5cqN6Zkh6wPaTeE/ljEVHlba9KzpC0qff8qWMkPRwRTxesd5Okh5qV0A5JNxasJdsnSrpM0rpOl9u8lA9ggPXDpjuAwgg6kABBBxIg6EACBB1IoK+CXvhwxlmrRT3qzXa9vgq6pJq/zKp/OOpRbzbr9VvQARRQ5IAZ2wN9FM7ChQun/T0HDx7UcccdN6N6ixZN/2S+vXv36tRTT51Rvf37p3/OzYEDBzR//vwZ1du9e/e0vyci1BwdN22HDh2a0ffNFRHxP7+YWT8Edi5atWpV1Xp33HFH1XpbtmypWm/DhuInhH3Fvn37qtbrB2y6AwkQdCABgg4kQNCBBAg6kABBBxIg6EACBB1IoFXQa45MAtC9KYPeXGTwD+pdgvY8SdfZPq90YwC602aNXnVkEoDutQl6mpFJwKBqc1JLq5FJzYnytc/ZBdBCm6C3GpkUERslbZQG/zRVYK5ps+k+0COTgAymXKPXHpkEoHutLjzRzAkrNSsMQGEcGQckQNCBBAg6kABBBxIg6EACBB1IgKADCRB0IAEmtcxA7ckpw8PDVevNZOTUN7F3796q9VavXl213sjISNV6k2GNDiRA0IEECDqQAEEHEiDoQAIEHUiAoAMJEHQgAYIOJEDQgQTajGS6z/aE7ddrNASge23W6H+UdEXhPgAUNGXQI+IFSXXPOgDQKfbRgQQ6O02V2WtA/+os6MxeA/oXm+5AAm3eXntE0t8kLbG9y/bPy7cFoEtthixeV6MRAOWw6Q4kQNCBBAg6kABBBxIg6EACBB1IgKADCRB0IIGBmL22fPnyqvVqz0I799xzq9bbsWNH1XqbN2+uWq/2/xdmrwGogqADCRB0IAGCDiRA0IEECDqQAEEHEiDoQAIEHUiAoAMJtLk45Fm2n7M9bvsN2zfXaAxAd9oc6/6FpN9ExFbbJ0sas705It4s3BuAjrSZvfZ+RGxt7n8qaVzSotKNAejOtPbRbZ8jaamkl4p0A6CI1qep2p4v6QlJ6yNi/yRfZ/Ya0KdaBd32PPVC/lBEPDnZY5i9BvSvNq+6W9K9ksYj4s7yLQHoWpt99JWSbpB0ie1tze3HhfsC0KE2s9delOQKvQAohCPjgAQIOpAAQQcSIOhAAgQdSICgAwkQdCABgg4kMBCz1xYuXFi13tjYWNV6tWeh1Vb795kRa3QgAYIOJEDQgQQIOpAAQQcSIOhAAgQdSICgAwkQdCABgg4k0OYqsMfbftn2q83stdtrNAagO22OdT8o6ZKIONBc3/1F23+JiL8X7g1AR9pcBTYkHWg+nNfcGNAAzCGt9tFtD9neJmlC0uaIYPYaMIe0CnpEHIqICyQtlnSh7fOPfozttbZHbY923COAb2har7pHxCeSnpd0xSRf2xgRKyJiRTetAehKm1fdT7e9oLl/gqRVkrYX7gtAh9q86n6mpAdsD6n3xPBYRDxVti0AXWrzqvs/JS2t0AuAQjgyDkiAoAMJEHQgAYIOJEDQgQQIOpAAQQcSIOhAAsxem4EtW7ZUrTfoav/99u3bV7VeP2CNDiRA0IEECDqQAEEHEiDoQAIEHUiAoAMJEHQgAYIOJEDQgQRaB70Z4vCKbS4MCcwx01mj3yxpvFQjAMppO5JpsaSrJN1Tth0AJbRdo98l6RZJX5ZrBUApbSa1XC1pIiLGpngcs9eAPtVmjb5S0jW2d0p6VNIlth88+kHMXgP615RBj4jbImJxRJwjaY2kZyPi+uKdAegM76MDCUzrUlIR8bx6Y5MBzCGs0YEECDqQAEEHEiDoQAIEHUiAoAMJEHQgAYIOJDAQs9dqz9Javnx51Xq11Z6FVvv3OTIyUrVeP2CNDiRA0IEECDqQAEEHEiDoQAIEHUiAoAMJEHQgAYIOJEDQgQRaHQLbXOr5U0mHJH3BJZ2BuWU6x7r/KCI+LtYJgGLYdAcSaBv0kPSM7THba0s2BKB7bTfdV0bEHtvfkbTZ9vaIeOHIBzRPADwJAH2o1Ro9IvY0/05I2iTpwkkew+w1oE+1maZ6ku2TD9+XdLmk10s3BqA7bTbdz5C0yfbhxz8cEU8X7QpAp6YMekTskPT9Cr0AKIS314AECDqQAEEHEiDoQAIEHUiAoAMJEHQgAYIOJOCI6H6hdvcL/RrDw8M1y2l0dLRqvXXr1lWtd+2111atV/vvt2LFYJ+OERE++nOs0YEECDqQAEEHEiDoQAIEHUiAoAMJEHQgAYIOJEDQgQQIOpBAq6DbXmD7cdvbbY/bvqh0YwC603aAw+8lPR0RP7V9rKQTC/YEoGNTBt32KZIulvQzSYqIzyV9XrYtAF1qs+k+LOkjSffbfsX2Pc0gh6+wvdb2qO26p3YBmFKboB8jaZmkuyNiqaTPJG04+kGMZAL6V5ug75K0KyJeaj5+XL3gA5gjpgx6RHwg6T3bS5pPXSrpzaJdAehU21fdb5L0UPOK+w5JN5ZrCUDXWgU9IrZJYt8bmKM4Mg5IgKADCRB0IAGCDiRA0IEECDqQAEEHEiDoQAIDMXuttrVr11atd+utt1atNzY2VrXe6tWrq9YbdMxeA5Ii6EACBB1IgKADCRB0IAGCDiRA0IEECDqQAEEHEpgy6LaX2N52xG2/7fUVegPQkSmvGRcRb0m6QJJsD0naLWlT2bYAdGm6m+6XSnonIt4t0QyAMqYb9DWSHinRCIByWge9uab7NZJG/s/Xmb0G9Km2Axwk6UpJWyPiw8m+GBEbJW2UBv80VWCumc6m+3Visx2Yk1oF3faJki6T9GTZdgCU0HYk078lfbtwLwAK4cg4IAGCDiRA0IEECDqQAEEHEiDoQAIEHUiAoAMJEHQggVKz1z6SNJNz1k+T9HHH7fRDLepRr1a9syPi9KM/WSToM2V7NCJWDFot6lFvtuux6Q4kQNCBBPot6BsHtBb1qDer9fpqHx1AGf22RgdQAEEHEiDoQAIEHUiAoAMJ/AchD47vy2xCkAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.gray()\n",
    "plt.matshow(digits.images[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c2c0b4",
   "metadata": {
    "id": "b1c2c0b4"
   },
   "source": [
    "#### OneHot Encoding Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "93e1f247",
   "metadata": {
    "id": "93e1f247"
   },
   "outputs": [],
   "source": [
    "# convert the labels with one hot encoding\n",
    "def oneHotEncoding(Y):\n",
    "    y_encoded = []\n",
    "    for i in range(Y.shape[0]):\n",
    "        y = np.zeros((10))\n",
    "        y[Y[i]] = 1\n",
    "        y_encoded.append(y)\n",
    "        \n",
    "    return np.array(y_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a88d7f86",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a88d7f86",
    "outputId": "1f6278d1-ca65-402d-e0bd-fb6b59d0915c"
   },
   "outputs": [],
   "source": [
    "# Enconding the labels\n",
    "y_encoded = oneHotEncoding(y)  # shape = (1797, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2921fe4",
   "metadata": {
    "id": "a2921fe4"
   },
   "source": [
    "#### Scaling and Splitting the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8568b914",
   "metadata": {
    "id": "8568b914"
   },
   "outputs": [],
   "source": [
    "# Scaling the data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_scaled = sc.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4b09fbaf",
   "metadata": {
    "id": "4b09fbaf"
   },
   "outputs": [],
   "source": [
    "# Splitting the data\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_encoded, test_size=0.33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bc884d69",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bc884d69",
    "outputId": "1ca8d542-bd53-4911-d4cf-ab314517799d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1203, 64)\n",
      "(1203, 10)\n",
      "(594, 64)\n",
      "(594, 10)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e3741e",
   "metadata": {
    "id": "c4e3741e"
   },
   "source": [
    "#### Implementation of the activation function and their derivative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "67ced4c3",
   "metadata": {
    "id": "67ced4c3"
   },
   "outputs": [],
   "source": [
    "def sigmoid(Z):\n",
    "    A = 1/(1 + np.exp(-Z))\n",
    "    return A\n",
    "\n",
    "def sigmoid_derivative(Z):\n",
    "    dA = sigmoid(Z)*(1 - sigmoid(Z))\n",
    "    return dA\n",
    "\n",
    "def tanh(Z):\n",
    "    A = np.tanh(Z)\n",
    "    return A\n",
    "\n",
    "def tanh_derivative(Z):\n",
    "    df = 1 - (tanh(Z)**2)\n",
    "    return df\n",
    "\n",
    "def ReLu(Z):\n",
    "    return np.maximum(0, Z)\n",
    "\n",
    "def ReLu_derivative(Z):\n",
    "    Z = np.where(Z < 0, 0, Z)\n",
    "    Z = np.where(Z >= 0, 1, Z)\n",
    "    return Z "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afababe2",
   "metadata": {
    "id": "afababe2"
   },
   "source": [
    "### NN: Forward and Backward Propagatation Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fc1b7a5a",
   "metadata": {
    "id": "fc1b7a5a"
   },
   "outputs": [],
   "source": [
    "# This is a 2 layer neural network - because we do calculations only for input-hidden and hidden-output\n",
    "\n",
    "# W is the weight matrix for layer l\n",
    "# b is the bias vector for layer l\n",
    "        \n",
    "# Z = W.X + b\n",
    "# g - the activation function applied on layer l\n",
    "# A - output after applying activation (g(Z))\n",
    "\n",
    "# L - number of layers, not including the input layer (2)\n",
    "# n - number of nodes in layers 1 (input: 64, hidden: 30, output: 10)\n",
    "# m - number of samples in the dataset\n",
    "\n",
    "\n",
    "# DIMENSIONS of the vectors and the arrays\n",
    "\n",
    "# W, dW - (# of nodes in l) x (# of nodes in l-1) [input-hidden: (30x64), hidden-output: (10x30)]\n",
    "# b, db - (# of nodes in l) x (1) [vector]\n",
    "# Z, dZ - (# of nodes in l) x (number of examples????)\n",
    "# A, dA - (# of nodes in l) x (number of examples????)\n",
    "\n",
    "class NNClassifier:\n",
    "    def __init__(self, n_layers, nn_structure, activation_func, derivation):\n",
    "        self.n_layers = n_layers\n",
    "        self.nn_structure = nn_structure\n",
    "        self.activation = activation_func\n",
    "        self.derivative = derivation  \n",
    "        \n",
    "    #initialize the parameters: the weight and the bias\n",
    "    def initialize_parameters(self):\n",
    "        W = {}\n",
    "        b = {}\n",
    "        for i in range(self.n_layers):\n",
    "            W[i] = np.random.randn(self.nn_structure[i+1], self.nn_structure[i]) * 0.01\n",
    "            b[i] = np.zeros((self.nn_structure[i+1], 1)) * 0.01\n",
    "            \n",
    "        return W, b\n",
    "        \n",
    "        \n",
    "    # forward propagatation - returns the linear and the activation function output for each layer   \n",
    "    def feed_forward(self, X, W, b):\n",
    "        cache = {}\n",
    "        A = X.T\n",
    "        \n",
    "        cache[\"Z0\"] = np.dot(W[0], X.T) + b[0]       # linear function output of hidden layer\n",
    "        cache[\"A0\"] = self.activation(cache[\"Z0\"])   # activation function output of hidden layer \n",
    "        \n",
    "        cache[\"Z1\"] = np.dot(W[1], cache[\"A0\"]) + b[1]  # linear function output of output layer\n",
    "        cache[\"A1\"] = self.activation(cache[\"Z1\"])      # activation function output of output layer - prediction\n",
    "        \n",
    "        return cache \n",
    "    \n",
    "    \n",
    "    # backward propgatation - returns dW and db for updating the parameter \n",
    "    def backward_propagation(self, cache, X, y, W, b):\n",
    "        dW = {}\n",
    "        db = {}\n",
    "        \n",
    "        # output - hidden layer        \n",
    "        dE = -2*(y.T - cache['A1'])          # (10, 1797) - MSE derivative\n",
    "        dA1 = self.derivative(cache['Z1'])   # (10, 1797) - activation function derivative\n",
    "        dZ1 = cache['A0']                    # (30, 1797) - linear function derivative\n",
    "        \n",
    "        delta0 = dE*dA1                      #(10,1797)  \n",
    "        delta1 = np.dot(delta0, dZ1.T)       #(10,1797) . (1797,30) = (10,30)   \n",
    "        \n",
    "        dW[1] = delta1                                          # (10, 30) - derivative of dW for output layer\n",
    "        db[1] = np.sum(delta0, axis = 1, keepdims = True)       # (10, 1)  - derivative of db for output layer\n",
    "        \n",
    "        # hidden - input layer \n",
    "        dZ2A1 = W[1]                            # (10,30) - linear function derivative with respect to A\n",
    "        dA1dZ1 = self.derivative(cache['Z0'])   # (30, 1797) - activation function derivative\n",
    "        dZ1 = X.T                               # (64,1797) - linear function derivative\n",
    "        \n",
    "        delta2 = np.dot(dZ1A0.T, delta0)        # (30, 10).(10,1797) = (30, 1797)\n",
    "        delta3 = delta2*dA0dZ0                  # (30, 1797)*(30,1797) = (30, 1797)\n",
    "        delta4 =  np.dot(delta3,dZ0.T)          # (30, 1797).(1797, 64) = (30, 64)\n",
    "        \n",
    "        dW[0] = delta4                                      # (30, 64) - derivative of dW for hidden layer\n",
    "        db[0] = np.sum(delta3, axis = 1, keepdims = True)   # (30, 1)  - derivative of db for hidden layer\n",
    "        \n",
    "        return dW, db\n",
    "\n",
    "    \n",
    "    def train_model(self, X, y, iterations, alpha):\n",
    "        \n",
    "        W, b  = self.initialize_parameters()   # randomly initialized parameters\n",
    "        \n",
    "        for epoch in range(iterations):\n",
    "            \n",
    "            cache = self.feed_forward(X, W, b)\n",
    "            dW, db = self.backward_propagation(cache, X, y, W, b)\n",
    "            \n",
    "            W[0] = W[0] - alpha * dW[0]/len(X)     # update W for each layer\n",
    "            b[0] = b[0] - alpha * db[0]/len(X)     # update b for each layer\n",
    "                \n",
    "            W[1] = W[1] - alpha * dW[1]/len(X)     # update W for each layer\n",
    "            b[1] = b[1] - alpha * db[1]/len(X)     # update b for each layer\n",
    "                \n",
    "        return W, b\n",
    "    \n",
    "    # returns the predicted values\n",
    "    def predict(self, X, W, b):\n",
    "        cache = self.feed_forward(X, W, b) \n",
    "        return cache[\"A1\"].T\n",
    "    \n",
    "    # checks the accuracy of the predicted value\n",
    "    def accuracy(self, y_predict, y_true):\n",
    "        count = 0\n",
    "        for i in range(y_true.shape[0]):\n",
    "            if np.argmax(y_predict[i]) == np.argmax(y_true[i]):\n",
    "                count = count + 1  \n",
    "        accuracy_score = count/y_true.shape[0] \n",
    "        return accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea0352d0",
   "metadata": {
    "id": "ea0352d0"
   },
   "source": [
    "### Training the Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "19c5da4f",
   "metadata": {
    "id": "19c5da4f"
   },
   "outputs": [],
   "source": [
    "nn_layers = 2               # number of layers in the neural network (excluding the input layer)\n",
    "nn_structure = [64, 30, 10] # number of neurons in [input, hidden, output] layers\n",
    "iterations = 1000           # iterations for training the model \n",
    "alpha = 0.6                 # learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "095b961f",
   "metadata": {
    "id": "095b961f"
   },
   "source": [
    "#### Sigmoid Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cebba389",
   "metadata": {
    "id": "cebba389"
   },
   "outputs": [],
   "source": [
    "nn_sigmoid = NNClassifier(nn_layers, nn_structure, sigmoid, sigmoid_derivative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b90b31e0",
   "metadata": {
    "id": "b90b31e0"
   },
   "outputs": [],
   "source": [
    "opt_W, opt_b = nn_sigmoid.train_model(X_train, y_train, iterations, alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "15e41e22",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "15e41e22",
    "outputId": "d05edfd1-23ee-46b2-8b52-ce31c17ae8ca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sigmoid Accuracy: 0.9646464646464646\n"
     ]
    }
   ],
   "source": [
    "y_predict_s = nn_sigmoid.predict(X_test, opt_W, opt_b)\n",
    "accuracy_score_s = nn_sigmoid.accuracy(y_predict_s, y_test)\n",
    "print(\"Sigmoid Accuracy:\", accuracy_score_s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8940117",
   "metadata": {
    "id": "e8940117"
   },
   "source": [
    "#### Tanh Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ab7bcc7f",
   "metadata": {
    "id": "ab7bcc7f"
   },
   "outputs": [],
   "source": [
    "nn_tanh = NNClassifier(nn_layers, nn_structure, tanh, tanh_derivative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3c4f2e60",
   "metadata": {
    "id": "3c4f2e60"
   },
   "outputs": [],
   "source": [
    "opt_W, opt_b = nn_tanh.train_model(X_train, y_train, iterations, alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d0800dc5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d0800dc5",
    "outputId": "9aa8d588-8191-420e-cf48-056672ed4624"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tanh Accuracy: 0.9579124579124579\n"
     ]
    }
   ],
   "source": [
    "y_predict_t = nn_tanh.predict(X_test, opt_W, opt_b)\n",
    "accuracy_score_t = nn_tanh.accuracy(y_predict_t, y_test)\n",
    "print(\"Tanh Accuracy:\", accuracy_score_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1934af57",
   "metadata": {
    "id": "1934af57"
   },
   "source": [
    "#### ReLu Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3f176165",
   "metadata": {
    "id": "3f176165"
   },
   "outputs": [],
   "source": [
    "nn_relu = NNClassifier(nn_layers, nn_structure, ReLu, ReLu_derivative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "662bf1a9",
   "metadata": {
    "id": "662bf1a9"
   },
   "outputs": [],
   "source": [
    "opt_W, opt_b = nn_relu.train_model(X_train, y_train, iterations, alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2220f2e3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2220f2e3",
    "outputId": "ae296c88-83af-4dae-e1a5-6bb3526d7889"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ReLu Accuracy: 0.09259259259259259\n"
     ]
    }
   ],
   "source": [
    "y_predict_r = nn_relu.predict(X_test, opt_W, opt_b)\n",
    "accuracy_score_r = nn_relu.accuracy(y_predict_r, y_test)\n",
    "print(\"ReLu Accuracy:\", accuracy_score_r)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f533ff62",
   "metadata": {
    "id": "f533ff62"
   },
   "source": [
    "#### Prediction and its accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b28cd583",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 143
    },
    "id": "b28cd583",
    "outputId": "fe123096-65e6-4870-dcf4-5fc8ed02fd9f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Activation Function</th>\n",
       "      <th>Accuracies</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sigmoid</td>\n",
       "      <td>0.964646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Tanh</td>\n",
       "      <td>0.957912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ReLu</td>\n",
       "      <td>0.092593</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Activation Function  Accuracies\n",
       "0             Sigmoid    0.964646\n",
       "1                Tanh    0.957912\n",
       "2                ReLu    0.092593"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracies = [[\"Sigmoid\", accuracy_score_s], [\"Tanh\", accuracy_score_t], [\"ReLu\", accuracy_score_r]]\n",
    "df = pd.DataFrame(accuracies, columns=['Activation Function', 'Accuracies'])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ea6ace",
   "metadata": {},
   "source": [
    "Based on the accuracy scores above, we can tell that the model with sigmoid as the activation function performs the best"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8af443ee",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5e2a8a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = [0.1, 0.01, 0.5, 0.6]\n",
    "iters = [1000, 10000, 5000, 50000]\n",
    "activations = [sigmoid, tanh, ReLu]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "50adea05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sigmoid - alpha: 0.1 - accuracy:  0.43265993265993263\n",
      "Sigmoid - alpha: 0.01 - accuracy:  0.08585858585858586\n",
      "Sigmoid - alpha: 0.5 - accuracy:  0.9545454545454546\n",
      "Sigmoid - alpha: 0.6 - accuracy:  0.9646464646464646\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(alpha)):\n",
    "    nn_s = NNClassifier(nn_layers, nn_structure, sigmoid, sigmoid_derivative)\n",
    "    opt_W, opt_b = nn_s.train_model(X_train, y_train, iters[0], alpha[i])\n",
    "    y_predict = nn_s.predict(X_test, opt_W, opt_b)\n",
    "    accuracy_score = nn_s.accuracy(y_predict, y_test)\n",
    "    print(\"Sigmoid - alpha:\", alpha[i], \"- accuracy: \", accuracy_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a66661fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tanh - alpha: 0.1 - accuracy:  0.9612794612794613\n",
      "Tanh - alpha: 0.01 - accuracy:  0.936026936026936\n",
      "Tanh - alpha: 0.5 - accuracy:  0.9663299663299664\n",
      "Tanh - alpha: 0.6 - accuracy:  0.9595959595959596\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(alpha)):\n",
    "    nn_t = NNClassifier(nn_layers, nn_structure, tanh, tanh_derivative)\n",
    "    opt_W, opt_b = nn_t.train_model(X_train, y_train, iters[0], alpha[i])\n",
    "    y_predict = nn_t.predict(X_test, opt_W, opt_b)\n",
    "    accuracy_score = nn_t.accuracy(y_predict, y_test)\n",
    "    print(\"Tanh - alpha:\", alpha[i], \"- accuracy: \", accuracy_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "922eb8ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ReLu - alpha: 0.1 - accuracy:  0.9663299663299664\n",
      "ReLu - alpha: 0.01 - accuracy:  0.9427609427609428\n",
      "ReLu - alpha: 0.5 - accuracy:  0.3383838383838384\n",
      "ReLu - alpha: 0.6 - accuracy:  0.09259259259259259\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(alpha)):\n",
    "    nn_r = NNClassifier(nn_layers, nn_structure, ReLu, ReLu_derivative)\n",
    "    opt_W, opt_b = nn_r.train_model(X_train, y_train, iters[0], alpha[i])\n",
    "    y_predict = nn_r.predict(X_test, opt_W, opt_b)\n",
    "    accuracy_score = nn_r.accuracy(y_predict, y_test)\n",
    "    print(\"ReLu - alpha:\", alpha[i], \"- accuracy: \", accuracy_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07086943",
   "metadata": {},
   "source": [
    "After testing with different hyperparamaters (learning rate and iterations), I decided to use learning rate = 0.5 and iterations = 1000 because they gave me the optimal accuracies for the three models above. "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
